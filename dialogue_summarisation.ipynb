{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogue Summarization using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this use case, we will be generating a summary of our dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face.\n",
    "\n",
    "We will use Zero-Shot, One-Shot and Few-Shot techniques. Then examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "import evaluate\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoModelForCausalLM, \n",
    "                          AutoTokenizer, GenerationConfig, TrainingArguments, Trainer)\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 4.65k/4.65k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 11.3M/11.3M [00:00<00:00, 14.6MB/s]\n",
      "Downloading data: 100%|██████████| 442k/442k [00:00<00:00, 2.18MB/s]]\n",
      "Downloading data: 100%|██████████| 1.35M/1.35M [00:00<00:00, 6.29MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 309.91it/s]\n",
      "Generating train split: 12460 examples [00:00, 28613.62 examples/s]\n",
      "Generating validation split: 500 examples [00:00, 8676.42 examples/s]\n",
      "Generating test split: 1500 examples [00:00, 29086.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Use the 'knkarthick/dialogsum' dataset from Hugging Face Datasets\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using two simple examples \n",
    "example_indices = [10, 70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "#Person1#: Tom, I've got good news for you.\n",
      "#Person2#: What is it?\n",
      "#Person1#: Haven't you heard that your novel has won The Nobel Prize?\n",
      "#Person2#: Really? I can't believe it. It's like a dream come true. I never expected that I would win The Nobel Prize!\n",
      "#Person1#: You did a good job. I'm extremely proud of you.\n",
      "#Person2#: Thanks for the compliment.\n",
      "#Person1#: You certainly deserve it. Let's celebrate!\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# tells Tom that his novel has won the Nobel Prize.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dash = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash)\n",
    "    print('Example', i+1)\n",
    "    print(dash)\n",
    "    print('Input Dialogue: ')\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash)\n",
    "    print('Baseline Human Summary: ')\n",
    "    print(dataset['test'][index]['summary'])\n",
    "    print(dash)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLAN- T5 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flan-PaLM 540B has been fine-tuned on a large number of tasks (over 1,800 tasks) using instruction-based learning. This helps the model generalize better to unseen tasks. It achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. With 540 billion parameters, it's one of the largest and most powerful language models available. \n",
    "\n",
    "\n",
    "Flan models excel at dialogue summarization due to their extensive instruction-based fine-tuning, which enhances their adaptability and performance on summarization tasks. They provide high-quality summaries even with limited examples, making them efficient and effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the 'google/flan-t5-base' model from Hugging Face Transformers\n",
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name) # Load the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) # use_fast=True to use fast tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example sentence\n",
    "input_text = \"Hey, can you help me with this problem?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Hey, can you help me with this problem?\n",
      "Encoded sentence: \n",
      " tensor([9459,    6,   54,   25,  199,  140,   28,   48,  682,   58,    1])\n",
      "Decoded sentence: Hey, can you help me with this problem?\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input text\n",
    "input_text_encoded = tokenizer(input_text, return_tensors='pt') # return_tensors='pt' to return PyTorch tensors\n",
    "input_text_decoded = tokenizer.decode(input_text_encoded['input_ids'][0], skip_special_tokens=True) # skip_special_tokens=True to remove special tokens\n",
    "\n",
    "print('Input Text:', input_text)\n",
    "print(f'Encoded sentence: \\n {input_text_encoded[\"input_ids\"][0]}')\n",
    "print('Decoded sentence:', input_text_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the summary\n",
    "def summarize_dialogues(example_indices, dataset, prompt = \"%s\"): # prompt = \"%s\" to prompt the model with the dialogue\n",
    "    for i, index in enumerate(example_indices):\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "\n",
    "        input = prompt % dialogue # Prompt the model with the dialogue\n",
    "        \n",
    "        inputs = tokenizer(input, return_tensors='pt') # Tokenize the input text\n",
    "        pred = model.generate(inputs['input_ids'], max_new_tokens=70)[0] # Generate the summary \n",
    "        output = tokenizer.decode(pred, skip_special_tokens=True) # Decode the summary\n",
    "\n",
    "        print(dash)\n",
    "        print('Example', i+1)\n",
    "        print(dash)\n",
    "        print('Input Dialogue: \\n ', dialogue) \n",
    "        print(dash)\n",
    "        print('Baseline Human Summary: \\n', summary)\n",
    "        print(dash)\n",
    "        print('Model Generated Summary: \\n', output)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "  #Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generated Summary: \n",
      " Brian, thank you for coming to our party.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "  #Person1#: Tom, I've got good news for you.\n",
      "#Person2#: What is it?\n",
      "#Person1#: Haven't you heard that your novel has won The Nobel Prize?\n",
      "#Person2#: Really? I can't believe it. It's like a dream come true. I never expected that I would win The Nobel Prize!\n",
      "#Person1#: You did a good job. I'm extremely proud of you.\n",
      "#Person2#: Thanks for the compliment.\n",
      "#Person1#: You certainly deserve it. Let's celebrate!\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# tells Tom that his novel has won the Nobel Prize.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generated Summary: \n",
      " Tom's novel has won the Nobel Prize.\n"
     ]
    }
   ],
   "source": [
    "summarize_dialogues(example_indices, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows room for improvement as the generated summaries could be more detailed and precise. While it does a fair job for example 2, its performance for example 1 can be more accurate and coherent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Inference with an Instruction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the following conversation.%s\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Summarize the following conversation.%s\" # Prompt the model with the dialogue \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "  #Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generated Summary: \n",
      " #Person1#: Happy birthday, Brian.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "  #Person1#: Tom, I've got good news for you.\n",
      "#Person2#: What is it?\n",
      "#Person1#: Haven't you heard that your novel has won The Nobel Prize?\n",
      "#Person2#: Really? I can't believe it. It's like a dream come true. I never expected that I would win The Nobel Prize!\n",
      "#Person1#: You did a good job. I'm extremely proud of you.\n",
      "#Person2#: Thanks for the compliment.\n",
      "#Person1#: You certainly deserve it. Let's celebrate!\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# tells Tom that his novel has won the Nobel Prize.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generated Summary: \n",
      " Tom's novel has won the Nobel Prize.\n"
     ]
    }
   ],
   "source": [
    "summarize_dialogues(example_indices, dataset, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What happened?%s\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What happened?%s\" \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "  #Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generated Summary: \n",
      " Brian's birthday party was held.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Input Dialogue: \n",
      "  #Person1#: Tom, I've got good news for you.\n",
      "#Person2#: What is it?\n",
      "#Person1#: Haven't you heard that your novel has won The Nobel Prize?\n",
      "#Person2#: Really? I can't believe it. It's like a dream come true. I never expected that I would win The Nobel Prize!\n",
      "#Person1#: You did a good job. I'm extremely proud of you.\n",
      "#Person2#: Thanks for the compliment.\n",
      "#Person1#: You certainly deserve it. Let's celebrate!\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      " #Person1# tells Tom that his novel has won the Nobel Prize.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generated Summary: \n",
      " Tom's novel has won the Nobel Prize.\n"
     ]
    }
   ],
   "source": [
    "summarize_dialogues(example_indices, dataset, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are slightly better after we gave it some prompts. But it can still be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the prompt\n",
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in example_indices_full:\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "        \n",
    "        prompt += f\"\"\"Dialogue:\\n{dialogue}\\n\\nWhat was going on?\\n{summary}\\n\\n\\n\"\"\"\n",
    "        dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "\n",
    "    prompt += f'Dialogue:\\n{dialogue}\\n\\nWhat was going on?'\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "\n",
      "What was going on?\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "What was going on?\n"
     ]
    }
   ],
   "source": [
    "#Giving example to model and giving a dialogue which needs to be summarized\n",
    "example_indices_full = [10]\n",
    "example_index_to_summarize = 80\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation - One Shot:\n",
      "Person1 wants to prepare for the picnic. May will help her prepare the food. Daniel will help her.\n"
     ]
    }
   ],
   "source": [
    "# Generate the summary\n",
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash)\n",
    "print(f'Baseline Human Summary: \\n{summary}\\n')\n",
    "print(dash)\n",
    "print(f'Model Generation - One Shot:\\n{output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to further improve our model by giving the model multiple examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue:\n",
      "#Person1#: Ms. Dawson, I need you to take a dictation for me.\n",
      "#Person2#: Yes, sir...\n",
      "#Person1#: This should go out as an intra-office memorandum to all employees by this afternoon. Are you ready?\n",
      "#Person2#: Yes, sir. Go ahead.\n",
      "#Person1#: Attention all staff... Effective immediately, all office communications are restricted to email correspondence and official memos. The use of Instant Message programs by employees during working hours is strictly prohibited.\n",
      "#Person2#: Sir, does this apply to intra-office communications only? Or will it also restrict external communications?\n",
      "#Person1#: It should apply to all communications, not only in this office between employees, but also any outside communications.\n",
      "#Person2#: But sir, many employees use Instant Messaging to communicate with their clients.\n",
      "#Person1#: They will just have to change their communication methods. I don't want any - one using Instant Messaging in this office. It wastes too much time! Now, please continue with the memo. Where were we?\n",
      "#Person2#: This applies to internal and external communications.\n",
      "#Person1#: Yes. Any employee who persists in using Instant Messaging will first receive a warning and be placed on probation. At second offense, the employee will face termination. Any questions regarding this new policy may be directed to department heads.\n",
      "#Person2#: Is that all?\n",
      "#Person1#: Yes. Please get this memo typed up and distributed to all employees before 4 pm.\n",
      "\n",
      "What was going on?\n",
      "Ms. Dawson takes a dictation for #Person1# about prohibiting the use of Instant Message programs in the office. They argue about its reasonability but #Person1# still insists.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: You're finally here! What took so long?\n",
      "#Person2#: I got stuck in traffic again. There was a terrible traffic jam near the Carrefour intersection.\n",
      "#Person1#: It's always rather congested down there during rush hour. Maybe you should try to find a different route to get home.\n",
      "#Person2#: I don't think it can be avoided, to be honest.\n",
      "#Person1#: perhaps it would be better if you started taking public transport system to work.\n",
      "#Person2#: I think it's something that I'll have to consider. The public transport system is pretty good.\n",
      "#Person1#: It would be better for the environment, too.\n",
      "#Person2#: I know. I feel bad about how much my car is adding to the pollution problem in this city.\n",
      "#Person1#: Taking the subway would be a lot less stressful than driving as well.\n",
      "#Person2#: The only problem is that I'm going to really miss having the freedom that you have with a car.\n",
      "#Person1#: Well, when it's nicer outside, you can start biking to work. That will give you just as much freedom as your car usually provides.\n",
      "#Person2#: That's true. I could certainly use the exercise!\n",
      "#Person1#: So, are you going to quit driving to work then?\n",
      "#Person2#: Yes, it's not good for me or for the environment.\n",
      "\n",
      "What was going on?\n",
      "#Person2# complains to #Person1# about the traffic jam, #Person1# suggests quitting driving and taking public transportation instead.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: This Olympic park is so big!\n",
      "#Person2#: Yes. Now we are in the Olympic stadium, the center of this park.\n",
      "#Person1#: Splendid! When is it gonna be finished?\n",
      "#Person2#: The whole stadium is to be finished this June.\n",
      "#Person1#: How many seats are there in the stand?\n",
      "#Person2#: Oh, there are 5000 seats in total.\n",
      "#Person1#: I didn ' t know it would be so big!\n",
      "#Person2#: It is! Look there, those are the tracks. And the jumping pit is over there.\n",
      "#Person1#: Ah... I see. Hey, look the sign here, No climbing.\n",
      "#Person2#: We put many signs with English translations for foreign visitors.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is surprised at the Olympic Stadium'volume, capacity and interior setting to #Person1#.\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "#Person1#: Tom, I've got good news for you.\n",
      "#Person2#: What is it?\n",
      "#Person1#: Haven't you heard that your novel has won The Nobel Prize?\n",
      "#Person2#: Really? I can't believe it. It's like a dream come true. I never expected that I would win The Nobel Prize!\n",
      "#Person1#: You did a good job. I'm extremely proud of you.\n",
      "#Person2#: Thanks for the compliment.\n",
      "#Person1#: You certainly deserve it. Let's celebrate!\n",
      "\n",
      "What was going on?\n"
     ]
    }
   ],
   "source": [
    "# Giving example to model and giving a dialogue which needs to be summarized\n",
    "example_indices_full = [2, 5, 12]\n",
    "example_index_to_summarize = 70\n",
    "\n",
    "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Baseline Human Summary: \n",
      "#Person1# tells Tom that his novel has won the Nobel Prize.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Model Generation - Few Shot:\n",
      "Tom's novel has won the Nobel Prize.\n"
     ]
    }
   ],
   "source": [
    "# Generate the summary\n",
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True\n",
    ")\n",
    "\n",
    "print(dash)\n",
    "print(f'Baseline Human Summary: \\n{summary}\\n')\n",
    "print(dash)\n",
    "print(f'Model Generation - Few Shot:\\n{output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
